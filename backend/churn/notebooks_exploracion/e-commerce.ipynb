{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from PreProcess import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .appName(\"churn_ecommerce\") \\\n",
    "      .config(\"spark.driver.memory\", \"4g\") \\\n",
    "      .config(\"spark.executor.memory\", \"4g\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def show_dataframe_summary(df, labels_col = None):\n",
    "    preprocess_instance = PreProcess(df)\n",
    "    categorical_cols = preprocess_instance.columns_types_atribute['categorical_cols']\n",
    "    numerical_cols = preprocess_instance.columns_types_atribute['numeric_cols']\n",
    "\n",
    "    # Show dtypes of each column\n",
    "    print(\"Dataframe schema: \\n\")\n",
    "    df.printSchema()\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # General overview\n",
    "    print(\"Showing top 5 rows: \\n\")\n",
    "    df.show(5)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Value counts of categoricals\n",
    "    if categorical_cols:\n",
    "        print(\"Showing value counts of each categorical feature: \\n\")\n",
    "        for c in categorical_cols:\n",
    "            df.groupBy(c).count().show()\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Overall data distribution of numerical columns\n",
    "    if numerical_cols:\n",
    "        print(\"Showing overall statistics of each numerical feature: \\n\")\n",
    "        df.select(numerical_cols).summary().show()\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # If labels_col is specified, show dataframe class distribution\n",
    "    if labels_col:\n",
    "        print(\"Showing dataset class distribution: \\n\")\n",
    "        df.groupBy(labels_col).count().show()\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Null counts and percentages per column\n",
    "    print(\"Showing null counts and percentages per column: \\n\")\n",
    "    null_counts, null_percentages = preprocess_instance.get_null_counts(df, with_percentages=True)\n",
    "    null_counts.show()\n",
    "    null_percentages.show()\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "def stratified_train_test_split(df, labels_col, train_ratio, seed):\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "    for label in df.select(labels_col).distinct().collect():\n",
    "        df_label = df.filter(df[labels_col] == label[0])\n",
    "\n",
    "        df_train, df_test = df_label.randomSplit([train_ratio, 1 - train_ratio], seed=seed)\n",
    "\n",
    "        train_dfs.append(df_train)\n",
    "        test_dfs.append(df_test)\n",
    "    \n",
    "    train_df_union = train_dfs[0]\n",
    "    test_df_union = test_dfs[0]\n",
    "    for i in range(1, len(train_dfs)):\n",
    "        train_df_union = train_df_union.union(train_dfs[i])\n",
    "        test_df_union = test_df_union.union(test_dfs[i])\n",
    "\n",
    "    return train_df_union, test_df_union\n",
    "\n",
    "\n",
    "def preprocess_data(df, labels_col, clean_nulls_options, transformation_categorical, transformation_numerical, stratified_split, train_split_ratio, resampling, resampling_options=None, seed=None):\n",
    "    preprocess_instance = PreProcess(df)\n",
    "    categorical_cols = preprocess_instance.columns_types_atribute['categorical_cols']\n",
    "    numerical_cols = preprocess_instance.columns_types_atribute['numeric_cols']\n",
    "\n",
    "    # Step 1: Clean null values\n",
    "    df = preprocess_instance.clean_nulls(df, **clean_nulls_options)\n",
    "\n",
    "    # Step 2: Clean duplicate rows\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    # Step 3: Process labels column\n",
    "    if labels_col in categorical_cols:\n",
    "        categorical_cols.remove(labels_col)\n",
    "        labels_column_mapping = [(labels_col, labels_col + \"_index\")]\n",
    "\n",
    "        df = preprocess_instance.string_index_columns(df, labels_column_mapping)\n",
    "        \n",
    "        df = df.drop(labels_col)\n",
    "    elif labels_col in numerical_cols:\n",
    "        numerical_cols.remove(labels_col)\n",
    "\n",
    "    # Step 4: Process categorical columns\n",
    "    if categorical_cols:\n",
    "        match transformation_categorical:\n",
    "            case 'index':\n",
    "                index_column_mapping = [(col, col + \"_index\") for col in categorical_cols]\n",
    "                df = preprocess_instance.string_index_columns(df, index_column_mapping)\n",
    "\n",
    "                df = df.drop(*categorical_cols)\n",
    "\n",
    "            case 'encode':\n",
    "                index_column_mapping = [(col, col + \"_index\") for col in categorical_cols]\n",
    "                df = preprocess_instance.string_index_columns(df, index_column_mapping)\n",
    "\n",
    "                encode_column_mapping = [(col + \"_index\", col + \"_encoded\") for col in categorical_cols]\n",
    "                df = preprocess_instance.encoded_index_columns(df, encode_column_mapping)\n",
    "\n",
    "                df = df.drop(*[col + \"_index\" for col in categorical_cols])\n",
    "                df = df.drop(*categorical_cols)\n",
    "\n",
    "    # Step 5: Process numerical columns\n",
    "    if numerical_cols:\n",
    "        match transformation_numerical:\n",
    "            case 'normalize':\n",
    "                df = preprocess_instance.normalize_min_max_values(numerical_cols, df)\n",
    "                df = df.drop(*numerical_cols)\n",
    "\n",
    "    # Step 6: Vectorize features\n",
    "    feature_cols = df.columns\n",
    "    feature_cols.remove(labels_col)\n",
    "    df = preprocess_instance.vector_feature_column(df, feature_cols, outputFeatureCols='features')\n",
    "    df = df.select(['features', labels_col])\n",
    "\n",
    "    # Step 6: Perform train/test split\n",
    "    if stratified_split:\n",
    "        train_df, test_df = stratified_train_test_split(df, labels_col, train_split_ratio, seed)\n",
    "    else:\n",
    "        train_df, test_df = df.randomSplit([train_split_ratio, 1-train_split_ratio], seed=seed)\n",
    "\n",
    "    # Step 5: Perform resampling techniques\n",
    "    match resampling:\n",
    "        case 'undersample_random':\n",
    "            train_df = preprocess_instance.undersample_random(train_df, labels_col)\n",
    "        case 'oversample_random':\n",
    "            train_df = preprocess_instance.oversample_random(train_df, labels_col)\n",
    "        case 'nearmiss_v2':\n",
    "            train_df = preprocess_instance.undersample_nearmiss_v2(train_df, 'features', labels_col, **resampling_options)\n",
    "        case 'class_weights':\n",
    "            train_df = preprocess_instance.compute_class_weights_column(df, labels_col)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into Spark Dataframe\n",
    "dataset_path = \"/home/jovyan/code/churn/datasets/E-Commerce_Dataset.csv\"\n",
    "df = spark.read.options(header=True, inferSchema=True, delimiter=';').csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overall summary of the dataset\n",
    "show_dataframe_summary(df, labels_col= 'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Deberíamos considerar pasos de Feature Engineering (i.e. creación de nuevas features)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing preprocess_data function\n",
    "clean_nulls_options = {\n",
    "    'row_threshold': 0.4,\n",
    "    'column_threshold': 0.2,\n",
    "    'default_values': {\n",
    "        'Tenure': 'median',\n",
    "        'WarehouseToHome': 'median',\n",
    "        'HourSpendOnApp': 'median',\n",
    "        'OrderAmountHikeFromlastYear': 'median',\n",
    "        'CouponUsed': 'median',\n",
    "        'OrderCount': 'median',\n",
    "        'DaySinceLastOrder': 'median'\n",
    "    }\n",
    "}\n",
    "train_df, test_df = preprocess_data(df, labels_col='Churn', clean_nulls_options=clean_nulls_options,\n",
    "                                    transformation_categorical='index', transformation_numerical=None,\n",
    "                                    stratified_split=False, train_split_ratio=0.8, resampling=None, seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataframe_summary(train_df, 'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataframe_summary(test_df, 'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model_instance = ClassificationModel(train_df, 'features', 'Churn')\n",
    "model = classification_model_instance.train_classification_model('Churn', 'features', None, 'RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[accuracy, precision, recall, f1_score] = classification_model_instance.performance_evaluation('Churn', 'prediction', predictions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
